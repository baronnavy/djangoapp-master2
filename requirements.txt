インフラ
第二技開の上原といいます。
最初に、簡単に我々が今どんな活動を行っているか説明します。
市品と開発で行っている活動で、市場データを活用する仕組み作りをしています。
【ポータルサイトを見せて】
LPBの市場データを集計して公開するポータルサイトを作っています。
JAM、故障エラー、寿命など、市場でどのように使われているかを可視化しています。
こうした色々なグラフを作っています。
【資料を見せて】
この取り組みの中で、市場データを管理するデータインフラ環境の構築も同時に進めています。
データインフラとしては、先ほどのポータルサイトのグラフをデータが追加されたら自動で更新していくという、自動化を実現したいです。
ただし、かなり簡易的な仕組みで構築したいと考えています。
本日、相談したい内容は、これから説明するデータインフラ環境に対して、アドバイスをいただきたいと思っています。
我々は、素人集団でこのようなインフラ構築をしようとしているので、些細なことでも良いので、色々教えて欲しいと思っています。
よろしくお願いします。
10分くらいで説明しようと思いますが、質問があれば、その場で止めてください。
こちらがそのデータの流れを示した図になっています。
今回扱っているデータは、AutoSendデータという本体のログデータになります。
HPから月に一回、csvファイル形式でキャノンに提供してもらっています。
なので月に一回、データを更新して、グラフを更新していくことになります。
Tableauの画面キャプチャというのが、ポータルサイトで見せているこのキャプチャになります。
青枠が仮想サーバーを示しており、その中に、csvファイルと各種計算をするPythonファイルがあります。
まずは、HPからもらった生データをここに保存します。
イメージとしては、こういう共有PC内に、各月ごとにフォルダが分けれており、フォルダ内に製品ごとのcsvファイルがあります。
これが毎月フォルダ増えていきます。
これが生データです。
これら生データを計算に使用するために、全てのデータを全期間で結合します。
こういったPythonのソースコードで、一個一個csvファイルを読み込んでは結合していき、製品ごとに一つのファイルに結合していきます。
最初は、製品ごとに、月毎のcsvファイルがあったものを、Pythonで結合して、製品ごとに一つのファイルに結合しています。
データが重いので、一つの製品で、10Gとかのサイズになるものも多々あります。
まだ、ここは、結合しただけの、生データです。
このままだと、Tableauで簡単に可視化できないので、Pythonで集計、計算して、Tableauの入力データにするcsvファイルを作ります。
ここをデータマートと呼んでいます。
このデータを直接Tableauで読み込んでキャプチャを取得して、ポータルサイトにアップします。

ざっくりとこんな流れになります。

実際にはもう少し細かく、入力データも一種類ではなく、複数あります。
ポータルサイトでいろんなグラフを作っているので、データマートも複数個あります。

白がcsvファイルで、黄色がPythonファイルを示しています。
これがデータマートです。

先ほど示したのは、このEventDataというファイルでした。月毎のデータを結合して、製品ごとにまとめます。それを入力として、JAMに関してのデータマートや、故障エラーに関するデータマートなど、作っています。
同様なものが、いくつかあります。

現状は、これらを手動で一個一個やって、エラーが出たらその都度修正しながら、なんとかやっている感じです。

これから、これらをできる限り簡易的な仕組みは維持したまま、ワンクリックで実行できるようにしていきたい。
基本は、バッチ実行で、これらを順番にやっていくことをイメージしている。
そのバッチをタスクスケジューラーで、月に一回実行させると良いのかな？と思っています。

ここから、相談内容になります。
現状、課題として感じていることは、こういったことがあります。

メモリ不足で計算できない製品とかもあって、高速化などもやらないといけないと感じており、アドバイスをいただきたいところになっています。
